{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIp1zAHiIKLOpm+2EkBbiJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shufen-Yin/Artificial-Intelligence/blob/main/Assignment_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "fXX1Hfww6KX4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqOTgaDmz_eU",
        "outputId": "b3fab13f-ef03-4985-bd9c-1b3926d6c59b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died too long ago for her to have more than an indistinct\n",
            "remembrance of her caresses; and her place had been supplied\n",
            "by an excellent woman as governess, who had fallen little short\n",
            "of a mother in affection.\n",
            "\n",
            "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
            "less as a governess than a friend, very fond of both daughters,\n",
            "but particularly of Emma.  Between _them_ it was more the intimacy\n",
            "of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
            "office of governess, the mildness o\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Task 1 Dataset Preparation:\n",
        "# Dataset Choice\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Load a sample text (e.g., Jane Austen's Emma)\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Check first 500 characters\n",
        "print(text[:500])\n",
        "\n",
        "\n",
        "# Load a sample text (e.g., Jane Austen)\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "print(text[:1000])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Text Preprocessing\n",
        "import re\n",
        "\n",
        "# Basic text cleaning\n",
        "text = text.lower()\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n"
      ],
      "metadata": {
        "id": "HwOMh37d0UZE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Visualization / Stats\n",
        "from collections import Counter\n",
        "words = text.split()\n",
        "print(\"Total words:\", len(words))\n",
        "print(\"Unique words:\", len(set(words)))\n",
        "print(\"Sample:\", words[:50])\n"
      ],
      "metadata": {
        "id": "fCHEUUqg0aWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5a68d4-08fe-493d-eb7b-e07e2d207cb5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 158128\n",
            "Unique words: 9306\n",
            "Sample: ['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twentyone', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'she', 'was']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 Exploring GPTs: Model Architecture**\n",
        "\n",
        "# 1 Transformer Model Overview\n",
        "\n",
        "GPT (Generative Pre-trained Transformer) is based on the Transformer architecture (Vaswani et al., 2017), which replaced RNNs for sequence modeling.\n",
        "\n",
        "Key features:\n",
        "\n",
        "Self-Attention Mechanism: Each token in the input sequence “attends” to other tokens, capturing context efficiently.\n",
        "\n",
        "Stacked Transformer Blocks: Each block has:\n",
        "\n",
        "Multi-head self-attention\n",
        "\n",
        "Feed-forward neural network\n",
        "\n",
        "Layer normalization and residual connections\n",
        "\n",
        "Decoder-only Architecture (GPT): GPT uses only the transformer decoder stack for causal language modeling, meaning it predicts the next token based on previous tokens.\n",
        "\n",
        "# Text Generation Process\n",
        "\n",
        "Tokenization: Converts text into tokens (words, subwords, or characters) using methods like Byte Pair Encoding (BPE).\n",
        "\n",
        "Probability Distribution: GPT outputs a probability for each token in the vocabulary for the next position.\n",
        "\n",
        "Sequence Generation:\n",
        "\n",
        "Start with a seed text (prompt).\n",
        "\n",
        "Iteratively sample the next token (using greedy, top-k, or nucleus sampling).\n",
        "\n",
        "Append it to the sequence and repeat until desired length is reached.\n",
        "\n",
        "Key idea: GPT predicts one token at a time using contextual embeddings from previous tokens."
      ],
      "metadata": {
        "id": "S-K6RSGH06hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2  Training\n",
        "# Implementing a Basic Text Generation Model\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Example: corpus\n",
        "corpus = text.lower().split()  #  split into words\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n"
      ],
      "metadata": {
        "id": "4Ngbu03-1OEu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert corpus tokens into token ids\n",
        "seq = [tokenizer.word_index[w] for w in corpus if w in tokenizer.word_index]\n",
        "\n",
        "# Choose a max sequence length\n",
        "max_seq_len = 40\n",
        "\n",
        "input_sequences = []\n",
        "\n",
        "# Sliding window to create sequences of fixed length\n",
        "for i in range(max_seq_len, len(seq)):\n",
        "    input_sequences.append(seq[i-max_seq_len:i+1])  # 40 tokens + 1 label token\n",
        "\n",
        "# Convert to numpy array\n",
        "input_sequences = np.array(input_sequences)\n",
        "\n",
        "# Split into inputs and labels\n",
        "X = input_sequences[:, :-1]   # 40 token\n",
        "y = input_sequences[:, -1]    # token predict\n"
      ],
      "metadata": {
        "id": "2Tjmm3CLyg92"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot label\n",
        "\n",
        "#from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# y = to_categorical(y, num_classes=total_words)\n"
      ],
      "metadata": {
        "id": "Xus995Bey9lk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_seq_len-1))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "2SL0WAvGzCnu",
        "outputId": "6ce571e7-c268-45ff-907c-0072df6799b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model.fit(X, y, epochs=50, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "i6u6a-UB544U",
        "outputId": "a2ebedd0-0cb6-4ad0-b73a-6cc39b877550"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2059284481.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 Application Demonstration: Content Generation with LSTM\n",
        "1️ Description (for report)\n",
        "\n",
        "The trained LSTM model can generate new text sequences based on a seed phrase. This demonstrates a content creation application, where the model predicts one word at a time to continue a text in the style of Jane Austen.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1 The user provides a seed text (e.g., \"emma woodhouse was\").\n",
        "\n",
        "2 The model predicts the next word using the learned probabilities from the training data.\n",
        "\n",
        "3 The predicted word is appended to the seed text.\n",
        "\n",
        "4 Steps 2–3 are repeated for a specified number of words (next_words).\n",
        "\n",
        "5 The output is a new, generated paragraph that follows the style and vocabulary of the original text.\n",
        "\n",
        "This can be used for:\n",
        "\n",
        "Generating creative writing content.\n",
        "\n",
        "Extending stories or chapters automatically.\n",
        "\n",
        "Text augmentation for NLP tasks."
      ],
      "metadata": {
        "id": "LbNlhJ-FBqF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def generate_text(seed_text, next_words, model, tokenizer, max_seq_len):\n",
        "    \"\"\"\n",
        "    Generate text using the trained LSTM model.\n",
        "\n",
        "    Parameters:\n",
        "    seed_text (str) : initial text to start generation\n",
        "    next_words (int) : number of words to generate\n",
        "    model : trained LSTM model\n",
        "    tokenizer : fitted Keras tokenizer\n",
        "    max_seq_len (int) : sequence length used during training\n",
        "\n",
        "    Returns:\n",
        "    str : generated text\n",
        "    \"\"\"\n",
        "    output_text = seed_text\n",
        "    for _ in range(next_words):\n",
        "        # Convert seed_text to sequence of token IDs\n",
        "        token_list = [tokenizer.word_index[w] for w in seed_text.lower().split() if w in tokenizer.word_index]\n",
        "\n",
        "        # Pad sequence\n",
        "        token_list = pad_sequences([token_list], maxlen=max_seq_len, padding='pre')\n",
        "\n",
        "        # Predict next word\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "        predicted = np.argmax(predicted_probs, axis=-1)[0]\n",
        "\n",
        "        # Find corresponding word\n",
        "        next_word = [word for word, index in tokenizer.word_index.items() if index == predicted][0]\n",
        "\n",
        "        # Append to text\n",
        "        seed_text += \" \" + next_word\n",
        "        output_text += \" \" + next_word\n",
        "\n",
        "    return output_text\n",
        "\n",
        "# -------------------------------\n",
        "# Example usage\n",
        "# -------------------------------\n",
        "seed = \"emma woodhouse was\"\n",
        "generated_text = generate_text(seed_text=seed, next_words=20, model=model, tokenizer=tokenizer, max_seq_len=max_seq_len-1)\n",
        "print(\"Generated Text:\\n\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "-QHEpZFGCKHt",
        "outputId": "a15dbd86-36db-4c77-deb3-a5b53a2d8d5c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2630064331.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"emma woodhouse was\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated Text:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}