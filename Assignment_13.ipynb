{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTgL3M/mm2c9zHzXHHe9z8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shufen-Yin/Artificial-Intelligence/blob/main/Assignment_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "fXX1Hfww6KX4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqOTgaDmz_eU",
        "outputId": "621bec8e-7b68-4cfa-b6ee-184d108d4e0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died too long ago for her to have more than an indistinct\n",
            "remembrance of her caresses; and her place had been supplied\n",
            "by an excellent woman as governess, who had fallen little short\n",
            "of a mother in affection.\n",
            "\n",
            "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
            "less as a governess than a friend, very fond of both daughters,\n",
            "but particularly of Emma.  Between _them_ it was more the intimacy\n",
            "of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
            "office of governess, the mildness o\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ],
      "source": [
        "# Task 1 Dataset Preparation:\n",
        "# Dataset Choice\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Load a sample text (e.g., Jane Austen's Emma)\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Check first 500 characters\n",
        "print(text[:500])\n",
        "\n",
        "\n",
        "# Load a sample text (e.g., Jane Austen)\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "print(text[:1000])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Text Preprocessing\n",
        "import nltk,re\n",
        "\n",
        "# Basic text cleaning\n",
        "text = text.lower()\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "# Example: load text (this is just an example with gutenberg)\n",
        "import nltk, re\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# cleanup\n",
        "text = text.lower()\n",
        "text = re.sub(r'[^a-z\\s]', ' ', text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHwoQY1F6_W3",
        "outputId": "ea4d1e7c-66e3-43f3-d293-e035e99b7f18"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Visualization / Stats\n",
        "from collections import Counter\n",
        "words = text.split()\n",
        "print(\"Total words:\", len(words))\n",
        "print(\"Unique words:\", len(set(words)))\n",
        "print(\"Sample:\", words[:50])\n"
      ],
      "metadata": {
        "id": "fCHEUUqg0aWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373fc751-d24e-47e0-eeee-057eef0e1dcb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 161975\n",
            "Unique words: 7096\n",
            "Sample: ['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'she']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 Exploring GPTs: Model Architecture**\n",
        "\n",
        "# 1 Transformer Model Overview\n",
        "\n",
        "GPT (Generative Pre-trained Transformer) is based on the Transformer architecture (Vaswani et al., 2017), which replaced RNNs for sequence modeling.\n",
        "\n",
        "Key features:\n",
        "\n",
        "Self-Attention Mechanism: Each token in the input sequence “attends” to other tokens, capturing context efficiently.\n",
        "\n",
        "Stacked Transformer Blocks: Each block has:\n",
        "\n",
        "Multi-head self-attention\n",
        "\n",
        "Feed-forward neural network\n",
        "\n",
        "Layer normalization and residual connections\n",
        "\n",
        "Decoder-only Architecture (GPT): GPT uses only the transformer decoder stack for causal language modeling, meaning it predicts the next token based on previous tokens.\n",
        "\n",
        "# Text Generation Process\n",
        "\n",
        "Tokenization: Converts text into tokens (words, subwords, or characters) using methods like Byte Pair Encoding (BPE).\n",
        "\n",
        "Probability Distribution: GPT outputs a probability for each token in the vocabulary for the next position.\n",
        "\n",
        "Sequence Generation:\n",
        "\n",
        "Start with a seed text (prompt).\n",
        "\n",
        "Iteratively sample the next token (using greedy, top-k, or nucleus sampling).\n",
        "\n",
        "Append it to the sequence and repeat until desired length is reached.\n",
        "\n",
        "Key idea: GPT predicts one token at a time using contextual embeddings from previous tokens."
      ],
      "metadata": {
        "id": "S-K6RSGH06hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2  Training\n",
        "# Implementing a Basic Text Generation Model\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. Prepare corpus\n",
        "corpus = text.split()  # split into words\n",
        "print(\"Total words in corpus:\", len(corpus))\n",
        "\n",
        "# 2. Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\", total_words)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4Ngbu03-1OEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3e516a-2d74-4b24-8a71-f9e6847f0029"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in corpus: 161975\n",
            "Vocabulary size: 7097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_ids = tokenizer.texts_to_sequences([\" \".join(corpus)])[0]\n",
        "# use only first 10000 tokens to build sequences\n",
        "small_ids = sequences_ids[:10000]\n",
        "max_len = 30\n",
        "\n",
        "input_sequences = []\n",
        "for i in range(max_len, len(small_ids)):\n",
        "    input_sequences.append(small_ids[i-max_len:i+1])\n",
        "\n",
        "print(\"Number sequences (small):\", len(input_sequences))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MPrunZF8Tkg",
        "outputId": "df4470e3-ad09-4e48-ab05-c7842db88aea"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number sequences (small): 9970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# If sequences are equal length\n",
        "max_seq_len = max(len(s) for s in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "print(\"input shape:\", input_sequences.shape, \"X shape:\", X.shape, \"y shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrdCFeA98YhL",
        "outputId": "7016812d-f72b-43bf-fc4b-735cef71ce1b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape: (9970, 31) X shape: (9970, 30) y shape: (9970,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "max_seq_len = max(len(s) for s in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
        "\n",
        "\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IfhNqZy8w1V",
        "outputId": "8e846e8a-24c9-4c6b-8118-b3b872d93f4c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (9970, 30)\n",
            "y shape: (9970,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce dataset size for faster training\n",
        "N = 8000   # you can try 2000 / 5000 / 8000\n",
        "X_small = X[:N]\n",
        "y_small = y[:N]\n",
        "\n",
        "print(\"X_small shape:\", X_small.shape)\n",
        "print(\"y_small shape:\", y_small.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN4FiJY1-9gq",
        "outputId": "9aa97c3b-6353-4c07-cb6f-6f418bc14ec8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_small shape: (8000, 30)\n",
            "y_small shape: (8000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build mode and train\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "vocab_size = total_words\n",
        "seq_len = X_small.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=seq_len))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_small, y_small, epochs=2, batch_size=32, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "E6kpesnT-pz1",
        "outputId": "f0dd7389-69b1-4d33-c92b-e11235547791"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - accuracy: 0.0293 - loss: 7.5599\n",
            "Epoch 2/2\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - accuracy: 0.0375 - loss: 6.0516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X, y, epochs=10, batch_size=32, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdwqlYbt_4qY",
        "outputId": "6345990e-8e81-48c2-a31b-2e07dd7e2ee6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.0398 - loss: 5.9853\n",
            "Epoch 2/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.0445 - loss: 5.8847\n",
            "Epoch 3/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.0562 - loss: 5.8088\n",
            "Epoch 4/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.0571 - loss: 5.7388\n",
            "Epoch 5/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 36ms/step - accuracy: 0.0543 - loss: 5.6760\n",
            "Epoch 6/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - accuracy: 0.0674 - loss: 5.5796\n",
            "Epoch 7/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.0687 - loss: 5.5322\n",
            "Epoch 8/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.0738 - loss: 5.4391\n",
            "Epoch 9/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.0872 - loss: 5.3348\n",
            "Epoch 10/10\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.0896 - loss: 5.2870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Increase model capacity\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=total_words, output_dim=64))  # bigger embedding\n",
        "model.add(LSTM(128))  # more units\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "max_len = 50  # gives more context\n",
        "\n"
      ],
      "metadata": {
        "id": "huMEOLMc_8vi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Generation Function\n",
        "\n",
        "def generate_text_temp(seed_text, next_words=30, temperature=1.0):\n",
        "    output_text = seed_text\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([output_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=X.shape[1], padding='pre')\n",
        "        preds = model.predict(token_list, verbose=0)[0]\n",
        "        preds = np.log(preds + 1e-8) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        next_index = np.random.choice(len(preds), p=preds)\n",
        "        next_word = tokenizer.index_word.get(next_index, \"\")\n",
        "        output_text += \" \" + next_word\n",
        "    return output_text\n",
        "\n",
        "print(generate_text_temp(\"emma was\", 50, temperature=0.7))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RudwEsOa_i54",
        "outputId": "0dd0e559-3067-4024-e0d7-71ad0196c87c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emma was pay churchill west scramble penitence therefore parts hasten panic relation become that joined absented over wisely wept groundless appears geography heat unpardonable clearing knight hasten flying clover explained demurs drunk passion demand understood indulgent slap convince delicacy struggled assuring paused bad eighteen and flirtation envy diverting dating impulse predominated mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "Small dataset (~9970 sequences) and lightweight LSTM (64 units, embedding 32) was enough for demonstration purposes.\n",
        "\n",
        "\n",
        "Text shows basic word-level coherence, though grammar isn’t perfect (expected in small toy models).\n",
        "\n",
        "Temperature sampling as a technique to improve diversity."
      ],
      "metadata": {
        "id": "X-gar-2zAwxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 Application Demonstration: Content Generation with LSTM\n",
        "1️ Description (for report)\n",
        "\n",
        "The trained LSTM model can generate new text sequences based on a seed phrase. This demonstrates a content creation application, where the model predicts one word at a time to continue a text in the style of Jane Austen.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1 The user provides a seed text (e.g., \"emma woodhouse was\").\n",
        "\n",
        "2 The model predicts the next word using the learned probabilities from the training data.\n",
        "\n",
        "3 The predicted word is appended to the seed text.\n",
        "\n",
        "4 Steps 2–3 are repeated for a specified number of words (next_words).\n",
        "\n",
        "5 The output is a new, generated paragraph that follows the style and vocabulary of the original text.\n",
        "\n",
        "This can be used for:\n",
        "\n",
        "Generating creative writing content.\n",
        "\n",
        "Extending stories or chapters automatically.\n",
        "\n",
        "Text augmentation for NLP tasks."
      ],
      "metadata": {
        "id": "LbNlhJ-FBqF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def generate_text(seed_text, next_words, model, tokenizer, max_seq_len):\n",
        "    \"\"\"\n",
        "    Generate text using the trained LSTM model.\n",
        "\n",
        "    Parameters:\n",
        "    seed_text (str) : initial text to start generation\n",
        "    next_words (int) : number of words to generate\n",
        "    model : trained LSTM model\n",
        "    tokenizer : fitted Keras tokenizer\n",
        "    max_seq_len (int) : sequence length used during training\n",
        "\n",
        "    Returns:\n",
        "    str : generated text\n",
        "    \"\"\"\n",
        "    output_text = seed_text\n",
        "    for _ in range(next_words):\n",
        "        # Convert seed_text to sequence of token IDs\n",
        "        token_list = [tokenizer.word_index[w] for w in seed_text.lower().split() if w in tokenizer.word_index]\n",
        "\n",
        "        # Pad sequence\n",
        "        token_list = pad_sequences([token_list], maxlen=max_seq_len, padding='pre')\n",
        "\n",
        "        # Predict next word\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "        predicted = np.argmax(predicted_probs, axis=-1)[0]\n",
        "\n",
        "        # Find corresponding word\n",
        "        next_word = [word for word, index in tokenizer.word_index.items() if index == predicted][0]\n",
        "\n",
        "        # Append to text\n",
        "        seed_text += \" \" + next_word\n",
        "        output_text += \" \" + next_word\n",
        "\n",
        "    return output_text\n",
        "\n",
        "# -------------------------------\n",
        "# Example usage\n",
        "# -------------------------------\n",
        "seed = \"emma woodhouse was\"\n",
        "generated_text = generate_text(seed_text=seed, next_words=20, model=model, tokenizer=tokenizer, max_seq_len=max_seq_len-1)\n",
        "print(\"Generated Text:\\n\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QHEpZFGCKHt",
        "outputId": "0fcb6e8b-f578-4ff4-9d3c-e8d1cb424cd9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " emma woodhouse was vacancies vacancies energy luxury iii parish prosings distresses ill resemblance wright cavil crayons consistently basis death advances advances looks sweetness\n"
          ]
        }
      ]
    }
  ]
}