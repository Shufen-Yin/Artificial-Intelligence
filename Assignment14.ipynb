{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shufen-Yin/Artificial-Intelligence/blob/main/Assignment14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhpwNnRKJZ-Y"
      },
      "outputs": [],
      "source": [
        "# Task 1 Data preparation\n",
        "#  1.1 Load dataset\n",
        "# import libraries\n",
        "!pip install fairlearn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from fairlearn.metrics import MetricFrame, selection_rate, false_positive_rate, true_positive_rate\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('adult.csv')\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)"
      ],
      "metadata": {
        "id": "1wjedmIFLKNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2 Basic cleaning: handle missing values\n",
        "\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# In the Adult dataset, missing values are as \"?\"\n",
        "df = df.replace(\"?\", np.nan)\n",
        "df = df.dropna()\n",
        "\n",
        "print(\"Dataset shape after dropping missing rows:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "jjcFNpKhJhaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "RCOzkhB0LZWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Preprocess the dataset by selecting features (X) and the target variable (y).\n",
        "# 2.1 Define target variable (y)\n",
        "# Convert income (<=50K, >50K) to binary 0/1\n",
        "df[\"income_binary\"] = df[\"income\"].apply(lambda x: 1 if \">50K\" in str(x) else 0)\n",
        "y = df[\"income_binary\"]\n",
        "\n",
        "print(\"Target value counts:\")\n",
        "print(y.value_counts())\n",
        "\n"
      ],
      "metadata": {
        "id": "W7MsQcXlQ_us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Define feature matrix (X)\n",
        "feature_cols = [\n",
        "    \"age\",\n",
        "    \"educational-num\",\n",
        "    \"hours-per-week\",\n",
        "    \"capital-gain\",\n",
        "    \"capital-loss\",\n",
        "    \"workclass\",\n",
        "    \"marital-status\",\n",
        "    \"occupation\",\n",
        "    \"relationship\",\n",
        "    \"race\",\n",
        "    \"gender\"\n",
        "]\n",
        "\n",
        "X = df[feature_cols]\n",
        "\n",
        "print(\"Feature columns:\")\n",
        "print(X.columns)\n",
        "X.head()\n"
      ],
      "metadata": {
        "id": "mPh_VwjiR1cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define sensitive attribute for fairness analysis\n",
        "# Here we use 'gender' as the sensitive attribute\n",
        "sensitive_attr = \"gender\"\n",
        "A = df[sensitive_attr]\n",
        "\n",
        "print(\"Sensitive attribute:\", sensitive_attr)\n",
        "print(A.value_counts())\n"
      ],
      "metadata": {
        "id": "jI2snFiES3fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 Model Training and Evaluation:\n",
        "# 1.1 Split the dataset into training and testing sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "UH_d30xTU7zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2 Preprocessing\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "numeric_features = [\n",
        "    \"age\",\n",
        "    \"educational-num\",\n",
        "    \"hours-per-week\",\n",
        "    \"capital-gain\",\n",
        "    \"capital-loss\"]\n",
        "\n",
        "categorical_features = [\n",
        "    \"workclass\",\n",
        "    \"marital-status\",\n",
        "    \"occupation\",\n",
        "    \"relationship\",\n",
        "    \"race\",\n",
        "    \"gender\"   ]\n",
        "\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)])\n"
      ],
      "metadata": {
        "id": "HCRCYQFUVbJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1Train a logistic regression model using scikit-learn.\n",
        "\n",
        "# Logistic Regression Model\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg_clf = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "log_reg_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training completed.\")\n"
      ],
      "metadata": {
        "id": "Rzmh6QjLVVUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Evaluationï¼šAccuracy + Confusion Matrix + Classification Report\n",
        "# Model Evaluation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "y_pred = log_reg_clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "2pig5YLAWhyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 Fairness analysis\n",
        "# 1.1 Fairness - Prepare sensitive feature on test set\n",
        "# We re-split gender along with X and y to keep them aligned\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sensitive_feature = df[\"gender\"]  # this is the sensitive attribute\n",
        "\n",
        "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    sensitive_feature,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"A_test value counts:\")\n",
        "print(A_test.value_counts())\n"
      ],
      "metadata": {
        "id": "5VDIgD77XbQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-fit the model after re-splitting\n",
        "log_reg_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "Ndw_6ZbiX3GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Fairlearn\n",
        "!pip install fairlearn\n"
      ],
      "metadata": {
        "id": "Xj9mkUaMX9ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fairness - Imports\n",
        "from fairlearn.metrics import (\n",
        "    MetricFrame,\n",
        "    selection_rate,\n",
        "    false_positive_rate,\n",
        "    true_positive_rate\n",
        ")\n"
      ],
      "metadata": {
        "id": "YkOTFUZbYJQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Fairness - Compute group metrics with MetricFrame\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Predicted labels on test set\n",
        "y_pred = log_reg_clf.predict(X_test)\n",
        "\n",
        "# Overall accuracy (should be close to 0.8547)\n",
        "overall_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Overall Test Accuracy:\", overall_accuracy)\n",
        "\n",
        "# Define metrics dictionary for MetricFrame\n",
        "metrics = {\n",
        "    \"selection_rate\": selection_rate,\n",
        "    \"false_positive_rate\": false_positive_rate,\n",
        "    \"true_positive_rate\": true_positive_rate,\n",
        "    \"accuracy\": accuracy_score\n",
        "}\n",
        "\n",
        "# Create MetricFrame grouped by gender (A_test)\n",
        "mf = MetricFrame(\n",
        "    metrics=metrics,\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred,\n",
        "    sensitive_features=A_test\n",
        ")\n",
        "\n",
        "print(\"Overall metrics (aggregated):\")\n",
        "print(mf.overall)\n",
        "\n",
        "print(\"\\nMetrics by gender group:\")\n",
        "print(mf.by_group)\n"
      ],
      "metadata": {
        "id": "Oe3gT1kfZeFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fairness - Custom bar plots with Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert by_group to DataFrame\n",
        "group_metrics = mf.by_group\n",
        "\n",
        "print(group_metrics)\n",
        "\n",
        "# Plot selection rate, FPR, TPR in one figure each\n",
        "for metric_name in [\"selection_rate\", \"false_positive_rate\", \"true_positive_rate\"]:\n",
        "    group_metrics[metric_name].plot(kind=\"bar\")\n",
        "    plt.title(f\"{metric_name} by gender\")\n",
        "    plt.xlabel(\"Gender\")\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "8Gg-sq_jafA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = group_metrics[\"accuracy\"].plot(kind=\"bar\")\n",
        "ax.set_title(\"accuracy by gender\")\n",
        "ax.set_xlabel(\"Gender\")\n",
        "ax.set_ylabel(\"accuracy\")\n"
      ],
      "metadata": {
        "id": "7SQ8SWk1NoMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 Explainability Analysis:\n",
        "# 1.1 Apply SHAP (SHapley Additive exPlanations) for global and local explainability\n",
        "#  Explainability - Install SHAP and LIME\n",
        "!pip install shap lime\n",
        "# ===== Explainability - Imports =====\n",
        "import shap"
      ],
      "metadata": {
        "id": "HOJSQqFqNr85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.2 SHAP - Prepare background data and feature names\n",
        "\n",
        "# Use training data as background; sample a subset for efficiency\n",
        "X_train_sample = X_train.sample(n=200, random_state=42)  # you can adjust n\n",
        "\n",
        "# Get feature names after preprocessing (for plots)\n",
        "# We need to fit the preprocessor separately to extract names\n",
        "preprocessor = log_reg_clf.named_steps[\"preprocessor\"]\n",
        "\n",
        "# Fit preprocessor on full training data (if not already fitted)\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "# Get transformed feature names (numeric + one-hot categorical)\n",
        "numeric_features = preprocessor.transformers_[0][2]\n",
        "categorical_features = preprocessor.transformers_[1][2]\n",
        "\n",
        "# OneHotEncoder is the second transformer\n",
        "ohe = preprocessor.named_transformers_[\"cat\"]\n",
        "ohe_feature_names = ohe.get_feature_names_out(categorical_features)\n",
        "\n",
        "all_feature_names = np.concatenate([numeric_features, ohe_feature_names])\n",
        "print(\"Number of features after preprocessing:\", len(all_feature_names))\n"
      ],
      "metadata": {
        "id": "1Z4nK0mdNy9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.3 SHAP - Build KernelExplainer for the pipeline\n",
        "\n",
        "# Define a wrapper to get probability of positive class from the pipeline\n",
        "\n",
        "# Safe model wrapper for DataFrame input\n",
        "feature_cols = X_train.columns  # original feature names\n",
        "\n",
        "def model_predict_proba(X_input):\n",
        "    \"\"\"\n",
        "    SHAP may pass a numpy array. We need to convert it back to a DataFrame\n",
        "    with the same column names as the original training data.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    if isinstance(X_input, np.ndarray):\n",
        "        X_df = pd.DataFrame(X_input, columns=feature_cols)\n",
        "    else:\n",
        "        # already a DataFrame\n",
        "        X_df = X_input\n",
        "\n",
        "    return log_reg_clf.predict_proba(X_df)[:, 1]\n",
        "\n",
        "# Create a KernelExplainer using a small background set\n",
        "X_train_sample = X_train.sample(n=50, random_state=42)  # keep as DataFrame\n",
        "\n",
        "explainer = shap.KernelExplainer(\n",
        "    model_predict_proba,\n",
        "    X_train_sample\n",
        ")"
      ],
      "metadata": {
        "id": "qYcxkxDrN5SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  SHAP - Compute shap_values for a test subset\n",
        "X_test_sample = X_test.sample(n=300, random_state=0)  # DataFrame\n",
        "\n",
        "shap_values = explainer.shap_values(X_test_sample, nsamples=100)\n",
        "\n",
        "# summary plot\n",
        "shap.summary_plot(\n",
        "    shap_values,\n",
        "    X_test_sample,\n",
        "    feature_names=X_test_sample.columns\n",
        ")\n"
      ],
      "metadata": {
        "id": "oBgAtFFDN9qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 Use LIME (Local Interpretable Model-agnostic Explanations) for detailed interpretation of individual predictions.\n",
        "#  LIME - Imports\n",
        "from lime.lime_tabular import LimeTabularExplainer"
      ],
      "metadata": {
        "id": "2nGWKr-oODUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2Identify numeric and categorical columns\n",
        "numeric_cols = ['age', 'educational-num', 'hours-per-week']  # adjust\n",
        "categorical_cols = [col for col in X_train.columns if col not in numeric_cols]\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "ohe = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
        "X_train_cat = ohe.fit_transform(X_train[categorical_cols])\n",
        "X_test_cat = ohe.transform(X_test[categorical_cols])\n",
        "\n",
        "# Concatenate numeric columns\n",
        "X_train_num = X_train[numeric_cols].values\n",
        "X_test_num = X_test[numeric_cols].values\n",
        "X_train_lime = np.hstack([X_train_num, X_train_cat])\n",
        "X_test_lime = np.hstack([X_test_num, X_test_cat])\n",
        "\n",
        "# Feature names\n",
        "feature_names = numeric_cols + list(ohe.get_feature_names_out(categorical_cols))\n"
      ],
      "metadata": {
        "id": "XDcT6LumOHAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3. Build LIME Tabular Explainer\n",
        "lime_explainer = LimeTabularExplainer(\n",
        "    training_data=X_train_lime,\n",
        "    feature_names=feature_names,\n",
        "    class_names=[\"<=50K\", \">50K\"],\n",
        "    discretize_continuous=True,\n",
        "    mode=\"classification\",\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "mkd37LstOKfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.4 Explain a single test instance\n",
        "# Train a Logistic Regression on manually one-hot encoded data\n",
        "\n",
        "log_reg_manual = LogisticRegression(max_iter=5000)\n",
        "log_reg_manual.fit(X_train_lime, y_train)  # use manually one-hot encoded X_train\n",
        "\n",
        "# 2.5 Explain a single test instance with LIME\n",
        "i = 0  # choose which test instance\n",
        "i = 0  # choose which test instance\n",
        "lime_exp = lime_explainer.explain_instance(\n",
        "    data_row=X_test_lime[i],          # use manually one-hot encoded test row\n",
        "    predict_fn=log_reg_manual.predict_proba,  # call the manually trained model\n",
        "    num_features=10\n",
        ")"
      ],
      "metadata": {
        "id": "HvuAm7WUOQd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print explanation\n",
        "print(\"LIME explanation for test instance\", i)\n",
        "print(lime_exp.as_list())\n"
      ],
      "metadata": {
        "id": "jpZgEWVYOTvt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}